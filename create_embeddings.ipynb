{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "f4gWgZi1VSE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torch\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # avoid CPU thread thrash on GPU runs\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Detect device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device:\", device, \"| GPU name:\", torch.cuda.get_device_name(0) if device=='cuda' else \"None\")\n",
        "\n",
        "# Load model ONCE, on the right device, with FP16 on GPU\n",
        "embedder = SentenceTransformer(\n",
        "    'Alibaba-NLP/gte-base-en-v1.5',\n",
        "    trust_remote_code=True,\n",
        "    device=device,\n",
        "    model_kwargs={\"torch_dtype\": torch.float16} if device == 'cuda' else {}\n",
        ")\n",
        "embedder.max_seq_length = 2048\n",
        "\n",
        "def create_and_save_embeddings(text_file: str, output_prefix: str, embedder=embedder):\n",
        "    if os.path.exists(f\"{output_prefix}_embeddings.npy\"):\n",
        "        print(f\"  ✓ Embeddings already exist for {text_file}, skipping...\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nProcessing {text_file}...\")\n",
        "\n",
        "    with open(text_file, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    texts = [a.strip() for a in content.split('</analysis>') if a.strip() and '<analysis>' in a]\n",
        "    if not texts:\n",
        "        print(f\"  ⚠ No analyses found in {text_file}\")\n",
        "        return\n",
        "\n",
        "    print(f\"  Creating embeddings for {len(texts)} analyses...\")\n",
        "\n",
        "    # Larger batch on T4; fall back if OOM\n",
        "    batch_size = 16 #if device == 'cuda' else 32\n",
        "    try:\n",
        "        # Keep on GPU during encode to avoid per-batch CPU copies\n",
        "        emb_t = embedder.encode(\n",
        "            texts,\n",
        "            batch_size=batch_size,\n",
        "            normalize_embeddings=True,\n",
        "            show_progress_bar=True,\n",
        "            convert_to_tensor=True,     # tensor on GPU\n",
        "        )\n",
        "    except RuntimeError as e:\n",
        "        if \"CUDA out of memory\" in str(e):\n",
        "            torch.cuda.empty_cache()\n",
        "            batch_size = 64 if device == 'cuda' else 16\n",
        "            print(f\"  ⚠ OOM detected. Retrying with batch_size={batch_size}...\")\n",
        "            emb_t = embedder.encode(\n",
        "                texts,\n",
        "                batch_size=batch_size,\n",
        "                normalize_embeddings=True,\n",
        "                show_progress_bar=True,\n",
        "                convert_to_tensor=True,\n",
        "            )\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "    # Move once to CPU at the end\n",
        "    embeddings = emb_t.detach().cpu().numpy().astype('float32')\n",
        "\n",
        "    # Build FAISS index (CPU is fine; encoding was the bottleneck)\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings)\n",
        "\n",
        "    np.save(f\"{output_prefix}_embeddings.npy\", embeddings)\n",
        "    faiss.write_index(index, f\"{output_prefix}_index.faiss\")\n",
        "    print(f\"  ✓ Saved embeddings and index\")\n",
        "\n",
        "# Run\n",
        "create_and_save_embeddings('equity_analyses.txt', 'morningstar_embeddings_equity')\n",
        "create_and_save_embeddings('fixed_income_analyses.txt', 'morningstar_embeddings_fixed_income')\n",
        "create_and_save_embeddings('allocation_analyses.txt', 'morningstar_embeddings_allocation')\n"
      ],
      "metadata": {
        "id": "EId9gKCrUmuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v8GPPtahaWAi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}